{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ../chapter04/data/neko.txt ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ./data/neko.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CaboCha\n",
    "https://qiita.com/nezuq/items/f481f07fc0576b38e81d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cabocha -f1 ./data/neko.txt > ./work/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat ./work/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
      "EOS\n",
      "EOS\n",
      "* 0 2D 0/0 -0.764522\n",
      "　\t記号,空白,*,*,*,*,　,　,　\n",
      "* 1 2D 0/1 -0.764522\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 2 -1D 0/2 0.000000\n",
      "\n",
      "-----\n",
      "\n",
      "南無阿弥陀仏\t名詞,一般,*,*,*,*,南無阿弥陀仏,ナムアミダブツ,ナムアミダブツ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "* 0 1D 0/0 0.000000\n",
      "ありがたい\t形容詞,自立,*,*,形容詞・アウオ段,基本形,ありがたい,アリガタイ,アリガタイ\n",
      "* 1 -1D 0/0 0.000000\n",
      "ありがたい\t形容詞,自立,*,*,形容詞・アウオ段,基本形,ありがたい,アリガタイ,アリガタイ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "!head ./work/neko.txt.cabocha\n",
    "!echo '\\n-----\\n'\n",
    "!tail ./work/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラス`Morph`を実装せよ．このクラスは表層形（`surface`），基本形（`base`），品詞（`pos`），品詞細分類1（`pos1`）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文を`Morph`オブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_repr\\_\\_ ??\n",
    "http://taustation.com/python3-str-repr/\n",
    "\n",
    "### @classmethod ??\n",
    "http://st-hakky.hatenablog.com/entry/2017/11/15/155523\n",
    "\n",
    "### cls ??\n",
    "http://hideharaaws.hatenablog.com/entry/2015/10/06/131848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby, islice\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    \n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Morph(surface : '{}', base : '{}', pos : '{}', pos1 : '{}')\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "    def contains_pos(self, pos_str):\n",
    "        if self.pos == pos_str:\n",
    "            return True\n",
    "        else: return False\n",
    "    \n",
    "    @classmethod\n",
    "    def extract(cls, line):\n",
    "        values = line.replace('\\t', ',', 1).split(',')\n",
    "        surface, pos, pos1, _, _, _, _, base, *_ = values\n",
    "        return cls(surface, base, pos, pos1)\n",
    "   \n",
    "    @classmethod\n",
    "    def morph_list_generator(cls, file):  \n",
    "        for not_eos, grp in groupby(file, lambda line: line != 'EOS\\n'):\n",
    "            if not_eos:\n",
    "                morph_list = []\n",
    "                for line in grp:\n",
    "                    if not line.startswith('*'):\n",
    "                        morph_list.append(cls.extract(line))\n",
    "                yield morph_list\n",
    "                \n",
    "    @classmethod\n",
    "    def morphs_of_sentenece(cls, file_path, n):\n",
    "        with open (file_path) as f:\n",
    "            for morphs in islice(Morph.morph_list_generator(f), n-1, n):\n",
    "                pp.pprint(morphs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom itertools import groupby\\n\\ndef morph_list_generator(file):  \\n    for not_eos, grp in groupby(file, lambda line: line != 'EOS\\n'):\\n        if not_eos:\\n            morph_list = []\\n            for line in grp:\\n                #if line[0] != '*':\\n                if not line.startswith('*'):   # startswithが使える\\n                    morph_list.append(Morph.extract(line))\\n            yield morph_list\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from itertools import groupby\n",
    "\n",
    "def morph_list_generator(file):  \n",
    "    for not_eos, grp in groupby(file, lambda line: line != 'EOS\\n'):\n",
    "        if not_eos:\n",
    "            morph_list = []\n",
    "            for line in grp:\n",
    "                #if line[0] != '*':\n",
    "                if not line.startswith('*'):   # startswithが使える\n",
    "                    morph_list.append(Morph.extract(line))\n",
    "            yield morph_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom itertools import islice\\nimport pprint as pp\\n\\nwith open ('work/neko.txt.cabocha') as f:\\n    for morphs in islice(Morph.morph_list_generator(f), 2, 3):\\n        pp.pprint(morphs)\\n\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from itertools import islice\n",
    "import pprint as pp\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for morphs in islice(Morph.morph_list_generator(f), 2, 3):\n",
    "        pp.pprint(morphs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスメソッドだけでやってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Morph(surface : '名前', base : '名前', pos : '名詞', pos1 : '一般'),\n",
      " Morph(surface : 'は', base : 'は', pos : '助詞', pos1 : '係助詞'),\n",
      " Morph(surface : 'まだ', base : 'まだ', pos : '副詞', pos1 : '助詞類接続'),\n",
      " Morph(surface : '無い', base : '無い', pos : '形容詞', pos1 : '自立'),\n",
      " Morph(surface : '。', base : '。', pos : '記号', pos1 : '句点')]\n"
     ]
    }
   ],
   "source": [
    "Morph.morphs_of_sentenece('work/neko.txt.cabocha', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラス`Chunk`を実装せよ．このクラスは形態素（`Morph`オブジェクト）のリスト（`morphs`），係り先文節インデックス番号（`dst`），係り元文節インデックス番号のリスト（`srcs`）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文を`Chunk`オブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    \n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Chunk(morphs : '{}', dst : '{}', srcs : '{}')\".format(self.morphs, self.dst, self.srcs)\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_chunks(cls, file):\n",
    "        for not_eos, sent in groupby(file, lambda line: line != 'EOS\\n'):\n",
    "            if not_eos:\n",
    "                chunk_list = []\n",
    "                dst_list = []\n",
    "                morphs_list = []\n",
    "                for not_star, words in groupby(sent, lambda line: not line.startswith('*')):\n",
    "                    if not_star:\n",
    "                        for word in words:\n",
    "                            # print(word)\n",
    "                            morph_list.append(Morph.extract(word))\n",
    "                        morphs_list.append(morph_list)\n",
    "                    else:\n",
    "                        morph_list = []\n",
    "                        for star in words:\n",
    "                            values = star.split(' ')\n",
    "                            dst_list.append(int(values[2].replace('D', '')))\n",
    "                flag = True\n",
    "            else:\n",
    "                if flag:\n",
    "                    srcs_list = []\n",
    "                    for i in range(len(dst_list)):\n",
    "                        srcs =[]\n",
    "                        for j, dst in enumerate(dst_list):\n",
    "                            if dst == i:\n",
    "                                srcs.append(j)\n",
    "                        srcs_list.append(srcs)\n",
    "                    for i in range(len(morphs_list)):\n",
    "                         chunk_list.append(Chunk(morphs_list[i], dst_list[i], srcs_list[i]))\n",
    "                    yield chunk_list\n",
    "                    chunk_list = []\n",
    "                    flag = False\n",
    "                    \n",
    "                    \n",
    "    @classmethod\n",
    "    def generate_dep_pairs(cls, file, start, end):\n",
    "        for i, chunks in enumerate(islice(cls.generate_chunks(file), start, end)):\n",
    "            dependency_list = []\n",
    "            words_dic = {}\n",
    "            for index, chunk in enumerate(chunks):\n",
    "                surface_sent = ''\n",
    "                for morph in chunk.morphs:\n",
    "                    if morph.pos != '記号':\n",
    "                        surface_sent += morph.surface\n",
    "                    words_dic[index] = surface_sent\n",
    "                    \n",
    "            for index, chunk in enumerate(chunks):\n",
    "                if chunk.dst == -1:\n",
    "                    dst_words = 'None'\n",
    "                else:\n",
    "                    dst_words = words_dic[chunk.dst]\n",
    "                dependency_list.append((words_dic[index], dst_words))\n",
    "            yield(dependency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Chunk` not found.\n"
     ]
    }
   ],
   "source": [
    "Chunk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk(morphs : '[Morph(surface : 'この', base : 'この', pos : '連体詞', pos1 : '*')]', dst : '1', srcs : '[]'),\n",
      " Chunk(morphs : '[Morph(surface : '書生', base : '書生', pos : '名詞', pos1 : '一般'), Morph(surface : 'という', base : 'という', pos : '助詞', pos1 : '格助詞'), Morph(surface : 'の', base : 'の', pos : '名詞', pos1 : '非自立'), Morph(surface : 'は', base : 'は', pos : '助詞', pos1 : '係助詞')]', dst : '7', srcs : '[0]'),\n",
      " Chunk(morphs : '[Morph(surface : '時々', base : '時々', pos : '副詞', pos1 : '一般')]', dst : '4', srcs : '[]'),\n",
      " Chunk(morphs : '[Morph(surface : '我々', base : '我々', pos : '名詞', pos1 : '代名詞'), Morph(surface : 'を', base : 'を', pos : '助詞', pos1 : '格助詞')]', dst : '4', srcs : '[]'),\n",
      " Chunk(morphs : '[Morph(surface : '捕え', base : '捕える', pos : '動詞', pos1 : '自立'), Morph(surface : 'て', base : 'て', pos : '助詞', pos1 : '接続助詞')]', dst : '5', srcs : '[2, 3]'),\n",
      " Chunk(morphs : '[Morph(surface : '煮', base : '煮る', pos : '動詞', pos1 : '自立'), Morph(surface : 'て', base : 'て', pos : '助詞', pos1 : '接続助詞')]', dst : '6', srcs : '[4]'),\n",
      " Chunk(morphs : '[Morph(surface : '食う', base : '食う', pos : '動詞', pos1 : '自立'), Morph(surface : 'という', base : 'という', pos : '助詞', pos1 : '格助詞')]', dst : '7', srcs : '[5]'),\n",
      " Chunk(morphs : '[Morph(surface : '話', base : '話', pos : '名詞', pos1 : 'サ変接続'), Morph(surface : 'で', base : 'だ', pos : '助動詞', pos1 : '*'), Morph(surface : 'ある', base : 'ある', pos : '助動詞', pos1 : '*'), Morph(surface : '。', base : '。', pos : '記号', pos1 : '句点')]', dst : '-1', srcs : '[1, 6]')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "import pprint as pp\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for chunks in islice(Chunk.generate_chunks(f), 7, 8):\n",
    "        pp.pprint(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : words -> dst\n",
      "0 : この -> 1\n",
      "1 : 書生というのは -> 7\n",
      "2 : 時々 -> 4\n",
      "3 : 我々を -> 4\n",
      "4 : 捕えて -> 5\n",
      "5 : 煮て -> 6\n",
      "6 : 食うという -> 7\n",
      "7 : 話である。 -> -1\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "print('index : words -> dst')\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for chunks in islice(Chunk.generate_chunks(f), 7, 8):\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            surface_sent = ''\n",
    "            for morph in chunk.morphs:\n",
    "                surface_sent += morph.surface\n",
    "            print(str(index) + ' : ' + surface_sent + ' -> ' + str(chunk.dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----sntence0-----\n",
      "一\tNone\n",
      "-----sntence1-----\n",
      "\t猫である\n",
      "吾輩は\t猫である\n",
      "猫である\tNone\n",
      "-----sntence2-----\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "無い\tNone\n",
      "-----sntence3-----\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "つかぬ\tNone\n",
      "-----sntence4-----\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "記憶している\tNone\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for i, chunks in enumerate(islice(Chunk.generate_chunks(f), 5)):\n",
    "        print('-----sntence' + str(i) + '-----')\n",
    "        words_dic = {}\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            surface_sent = ''\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos != '記号':\n",
    "                    surface_sent += morph.surface\n",
    "                words_dic[index] = surface_sent\n",
    "        # print(words_dic)\n",
    "        \n",
    "        for index, chunk in enumerate(chunks):\n",
    "            if chunk.dst == -1:\n",
    "                dst_words = 'None'\n",
    "            else:\n",
    "                dst_words = words_dic[chunk.dst]\n",
    "            print(words_dic[index] + '\\t' + dst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----sntence0-----\n",
      "一\tNone\n",
      "-----sntence1-----\n",
      "\t猫である\n",
      "吾輩は\t猫である\n",
      "猫である\tNone\n",
      "-----sntence2-----\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "無い\tNone\n",
      "-----sntence3-----\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "つかぬ\tNone\n",
      "-----sntence4-----\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "記憶している\tNone\n"
     ]
    }
   ],
   "source": [
    "# クラスメソッドで\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for index, dep_pairs in enumerate(Chunk.generate_dep_pairs(f, 0, 5)):\n",
    "        print('-----sntence' + str(index) + '-----')\n",
    "        for dep_pair in dep_pairs:\n",
    "            print(dep_pair[0] + '\\t' + dep_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----sntence0-----\n",
      "-----sntence1-----\n",
      "-----sntence2-----\n",
      "-----sntence3-----\n",
      "どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "-----sntence4-----\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "-----sntence5-----\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "-----sntence6-----\n",
      "あとで\t聞くと\n",
      "-----sntence7-----\n",
      "我々を\t捕えて\n",
      "-----sntence8-----\n",
      "-----sntence9-----\n",
      "掌に\t載せられて\n",
      "スーと\t持ち上げられた\n",
      "時\tフワフワした\n",
      "感じが\tあったばかりである\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for i, chunks in enumerate(islice(Chunk.generate_chunks(f), 10)):\n",
    "        print('-----sntence' + str(i) + '-----')\n",
    "        words_dic = {}\n",
    "        pos_list_dic = {}\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            surface_sent = ''\n",
    "            pos_list = []\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos != '記号':\n",
    "                    surface_sent += morph.surface\n",
    "                    pos_list.append(morph.pos)\n",
    "                words_dic[index] = surface_sent\n",
    "                pos_list_dic[index] = pos_list\n",
    "        # print(words_dic)\n",
    "        # print(pos_list_dic)\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            if chunk.dst != -1:\n",
    "                 if '名詞' in pos_list_dic[index] and '動詞' in pos_list_dic[chunk.dst]:\n",
    "                    print(words_dic[index] + '\\t' + words_dic[chunk.dst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphviz\n",
    "http://graphviz.readthedocs.io/en/stable/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work/hello.gv.pdf'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "g = Digraph('G', filename = 'work/hello.gv')\n",
    "g.edge('Hello', 'World')\n",
    "g.edge('test', 'World')\n",
    "g.view()   #pdf出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"140pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 139.75 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-112 135.7542,-112 135.7542,4 -4,4\"/>\n",
       "<!-- Hello -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Hello</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"29.7542\" cy=\"-90\" rx=\"29.5104\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"29.7542\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Hello</text>\n",
       "</g>\n",
       "<!-- World -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>World</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"66.7542\" cy=\"-18\" rx=\"31.9905\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.7542\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">World</text>\n",
       "</g>\n",
       "<!-- Hello&#45;&gt;World -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Hello&#45;&gt;World</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M38.7108,-72.5708C43.0417,-64.1431 48.3336,-53.8455 53.1487,-44.4755\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.294,-46.0123 57.7517,-35.5182 50.068,-42.8128 56.294,-46.0123\"/>\n",
       "</g>\n",
       "<!-- test -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>test</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"104.7542\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.7542\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">test</text>\n",
       "</g>\n",
       "<!-- test&#45;&gt;World -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>test&#45;&gt;World</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M95.7487,-72.937C91.2714,-64.4537 85.759,-54.0092 80.7471,-44.513\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"83.722,-42.6509 75.959,-35.4407 77.5313,-45.9182 83.722,-42.6509\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x10a4c40b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 42を書き換えてペアで保存する．\n",
    "from itertools import islice\n",
    "\n",
    "dependency_list = []\n",
    "\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for i, chunks in enumerate(islice(Chunk.generate_chunks(f), 4, 5)):\n",
    "        words_dic = {}\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            surface_sent = ''\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos != '記号':\n",
    "                    surface_sent += morph.surface\n",
    "                words_dic[index] = surface_sent\n",
    "        # print(words_dic)\n",
    "        \n",
    "        for index, chunk in enumerate(chunks):\n",
    "            if chunk.dst == -1:\n",
    "                dst_words = 'None'\n",
    "            else:\n",
    "                dst_words = words_dic[chunk.dst]\n",
    "            dependency_list.append((words_dic[index], dst_words))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスメソッドで\n",
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for dep_pairs in Chunk.generate_dep_pairs(f, 4, 5):\n",
    "        dependency_list = dep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('何でも', '薄暗い'),\n",
       " ('薄暗い', '所で'),\n",
       " ('じめじめした', '所で'),\n",
       " ('所で', '泣いて'),\n",
       " ('ニャーニャー', '泣いて'),\n",
       " ('泣いて', '記憶している'),\n",
       " ('いた事だけは', '記憶している'),\n",
       " ('記憶している', 'None')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work/44.gv.pdf'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "g44 = Digraph('G', filename = 'work/44.gv')\n",
    "for pair in dependency_list:\n",
    "    if pair[1] != 'None':\n",
    "        g44.edge(pair[0], pair[1])\n",
    "g44.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"374pt\" height=\"357pt\"\n",
       " viewBox=\"0.00 0.00 373.72 357.06\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 353.0613)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-353.0613 369.7229,-353.0613 369.7229,4 -4,4\"/>\n",
       "<!-- 何でも -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>何でも</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"41.0122\" cy=\"-328.5551\" rx=\"41.0244\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.0122\" y=\"-327.8552\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">何でも</text>\n",
       "</g>\n",
       "<!-- 薄暗い -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>薄暗い</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"41.0122\" cy=\"-251.5429\" rx=\"41.0244\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.0122\" y=\"-250.8429\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">薄暗い</text>\n",
       "</g>\n",
       "<!-- 何でも&#45;&gt;薄暗い -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>何でも&#45;&gt;薄暗い</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M41.0122,-307.9173C41.0122,-300.0617 41.0122,-290.9314 41.0122,-282.3357\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"44.5123,-282.2377 41.0122,-272.2377 37.5123,-282.2378 44.5123,-282.2377\"/>\n",
       "</g>\n",
       "<!-- 所で -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>所で</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"106.0122\" cy=\"-174.5306\" rx=\"31.2258\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"106.0122\" y=\"-173.8307\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">所で</text>\n",
       "</g>\n",
       "<!-- 薄暗い&#45;&gt;所で -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>薄暗い&#45;&gt;所で</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M57.0796,-232.5061C65.2881,-222.7807 75.3945,-210.8066 84.2873,-200.2704\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"87.0242,-202.4541 90.7995,-192.5547 81.6749,-197.9391 87.0242,-202.4541\"/>\n",
       "</g>\n",
       "<!-- 泣いて -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>泣いて</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"166.0122\" cy=\"-97.5184\" rx=\"40.7328\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.0122\" y=\"-96.8184\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">泣いて</text>\n",
       "</g>\n",
       "<!-- 所で&#45;&gt;泣いて -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>所で&#45;&gt;泣いて</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M120.2314,-156.2798C127.5596,-146.8738 136.6441,-135.2135 144.7839,-124.7657\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"147.6729,-126.7525 151.0578,-116.7129 142.1509,-122.4503 147.6729,-126.7525\"/>\n",
       "</g>\n",
       "<!-- じめじめした -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>じめじめした</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"171.0122\" cy=\"-251.5429\" rx=\"70.922\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.0122\" y=\"-250.8429\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">じめじめした</text>\n",
       "</g>\n",
       "<!-- じめじめした&#45;&gt;所で -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>じめじめした&#45;&gt;所で</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M153.9342,-231.3088C145.9207,-221.8143 136.2786,-210.3904 127.7509,-200.2867\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"130.3277,-197.9133 121.2031,-192.5289 124.9784,-202.4283 130.3277,-197.9133\"/>\n",
       "</g>\n",
       "<!-- 記憶している -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>記憶している</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"230.0122\" cy=\"-20.5061\" rx=\"69.6399\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.0122\" y=\"-19.8061\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">記憶している</text>\n",
       "</g>\n",
       "<!-- 泣いて&#45;&gt;記憶している -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>泣いて&#45;&gt;記憶している</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M181.8324,-78.4816C189.3681,-69.4139 198.5282,-58.3913 206.8297,-48.402\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.5561,-50.5974 213.2557,-40.6694 204.1725,-46.1234 209.5561,-50.5974\"/>\n",
       "</g>\n",
       "<!-- ニャーニャー -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>ニャーニャー</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"226.0122\" cy=\"-174.5306\" rx=\"70.922\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"226.0122\" y=\"-173.8307\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ニャーニャー</text>\n",
       "</g>\n",
       "<!-- ニャーニャー&#45;&gt;泣いて -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>ニャーニャー&#45;&gt;泣いて</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M210.2479,-154.2966C203.2658,-145.3348 194.9444,-134.654 187.4129,-124.987\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"189.958,-122.5588 181.0511,-116.8214 184.4361,-126.861 189.958,-122.5588\"/>\n",
       "</g>\n",
       "<!-- いた事だけは -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>いた事だけは</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"295.0122\" cy=\"-97.5184\" rx=\"70.922\" ry=\"20.5123\"/>\n",
       "<text text-anchor=\"middle\" x=\"295.0122\" y=\"-96.8184\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">いた事だけは</text>\n",
       "</g>\n",
       "<!-- いた事だけは&#45;&gt;記憶している -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>いた事だけは&#45;&gt;記憶している</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M277.9342,-77.2843C270.4702,-68.4409 261.5934,-57.9236 253.5201,-48.3583\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"256.1007,-45.9895 246.9761,-40.6051 250.7514,-50.5045 256.1007,-45.9895\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x10ac10668>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい．\n",
    "動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ．\n",
    "ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "+ 述語に係る助詞を格とする\n",
    "+ 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "+ コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "+ 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('work/neko.txt.cabocha') as f:\n",
    "    for i, chunks in enumerate(islice(Chunk.generate_chunks(f), 10)):\n",
    "        \"\"\"\n",
    "        print('-----sntence' + str(i) + '-----')\n",
    "        words_dic = {}\n",
    "        pos_list_dic = {}\n",
    "        \"\"\"\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos != '動詞':\n",
    "                    surface_sent += morph.surface\n",
    "                    pos_list.append(morph.pos)\n",
    "                words_dic[index] = surface_sent\n",
    "                pos_list_dic[index] = pos_list\n",
    "        # print(words_dic)\n",
    "        # print(pos_list_dic)\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            if chunk.dst != -1:\n",
    "                 if '名詞' in pos_list_dic[index] and '動詞' in pos_list_dic[chunk.dst]:\n",
    "                    print(words_dic[index] + '\\t' + words_dic[chunk.dst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "+ 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える．\n",
    "この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "+ 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "+ 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "+ 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "+ 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "+ コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "+ コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ．\n",
    "ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "+ 各文節は（表層形の）形態素列で表現する\n",
    "+ パスの開始文節から終了文節に至るまで，各文節の表現を\"` -> `\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が$i$と$j$（$i < j$）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "+ 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"` -> `\"で連結して表現する\n",
    "+ 文節$i$と$j$に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "+ 文節$i$から構文木の根に至る経路上に文節$j$が存在する場合: 文節$i$から文節$j$のパスを表示\n",
    "+ 上記以外で，文節$i$と文節$j$から構文木の根に至る経路上で共通の文節$k$で交わる場合: 文節$i$から文節$k$に至る直前のパスと文節$j$から文節$k$に至る直前までのパス，文節$k$の内容を\"` | `\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
