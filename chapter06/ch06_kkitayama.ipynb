{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 英語テキストの処理\n",
    "英語のテキスト（[nlp.txt](data/nlp.txt)）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wgetには-ncコマンド（上書き防止）をつけよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ファイル `./data/nlp.txt' はすでに存在するので、取得しません。\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt -P ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\r\n",
      "From Wikipedia, the free encyclopedia\r\n",
      "\r\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humani-computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\r\n",
      "\r\n",
      "History\r\n",
      "\r\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\r\n",
      "\r\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\r\n"
     ]
    }
   ],
   "source": [
    "!head ./data/nlp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  先読み後読み？\n",
    "横井さんの : https://io-lab.esa.io/posts/450\n",
    "\n",
    "ネットの : https://abicky.net/2010/05/30/135112/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sent_generator(f):\n",
    "    for line in f:\n",
    "        for sent in re.split(r'(?<=[.:;?!])\\s+(?=[A-Z])', line.rstrip()):   # \"\\s\"よりも\"\\s+\"の方が綺麗ではないテキストにも対応できる．\n",
    "            if sent: yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom itertools import islice\\n\\nwith open('./data/nlp.txt', 'r') as f:\\n    for sent in islice(sent_generator(f), 5):\\n        print(sent)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from itertools import islice\n",
    "\n",
    "with open('./data/nlp.txt', 'r') as f:\n",
    "    for sent in islice(sent_generator(f), 5):\n",
    "        print(sent)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/nlp.txt', 'r') as f:\n",
    "    with open('./work/q50.txt', 'w') as fw:\n",
    "        for sent in sent_generator(f):\n",
    "            fw.write(sent+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\r\n",
      "From Wikipedia, the free encyclopedia\r\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\r\n",
      "As such, NLP is related to the area of humani-computer interaction.\r\n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ./work/q50.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑\"Natural language processing\", \"From Wikipedia, the free encyclopedia\"は，splitの正規表現にマッチせずそのまま出てくる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./work/q50.txt', 'r') as f:\n",
    "    with open('./work/q51.txt', 'w') as fw:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                fw.write(word.strip(',.?!\"') + '\\n')\n",
    "            fw.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\r\n",
      "language\r\n",
      "processing\r\n",
      "\r\n",
      "From\r\n",
      "Wikipedia\r\n",
      "the\r\n",
      "free\r\n",
      "encyclopedia\r\n",
      "\r\n",
      "Natural\r\n",
      "language\r\n",
      "processing\r\n",
      "(NLP)\r\n",
      "is\r\n",
      "a\r\n",
      "field\r\n",
      "of\r\n",
      "computer\r\n",
      "science\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 ./work/q51.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ．\n",
    "Pythonでは，Porterのステミングアルゴリズムの実装として[stemming](https://pypi.python.org/pypi/stemming)モジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【python】nltkで英語のStemmingとLemmatization : https://hayataka2049.hatenablog.jp/entry/2018/03/25/203836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tnatural\n",
      "language\tlanguage\n",
      "processing\tprocessing\n",
      "\t\n",
      "From\tfrom\n",
      "Wikipedia\twikipedia\n",
      "the\tthe\n",
      "free\tfree\n",
      "encyclopedia\tencyclopedia\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer as PS\n",
    "from itertools import islice\n",
    "\n",
    "ps = PS()\n",
    "with open('./work/q51.txt', 'r') as f:\n",
    "    for word in islice(f, 10):\n",
    "        print(word.rstrip()+'\\t'+ps.stem(word), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. Tokenization\n",
    "[Stanford Core NLP](http://nlp.stanford.edu/software/corenlp.shtml)を用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/Users/kitayama/CoreNLP/stanford-corenlp-full-2018-02-27/corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file ./data/nlp.txt -outputDirectory ./work/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xml.etree.ElementTree\n",
    "https://docs.python.jp/3/library/xml.etree.elementtree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r",
      "\r\n",
      "<?xml-stylesheet href=\"CoreNLP-to-HTML.xsl\" type=\"text/xsl\"?>\r",
      "\r\n",
      "<root>\r",
      "\r\n",
      "  <document>\r",
      "\r\n",
      "    <docId>nlp.txt</docId>\r",
      "\r\n",
      "    <sentences>\r",
      "\r\n",
      "      <sentence id=\"1\">\r",
      "\r\n",
      "        <tokens>\r",
      "\r\n",
      "          <token id=\"1\">\r",
      "\r\n",
      "            <word>Natural</word>\r",
      "\r\n",
      "            <lemma>natural</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>0</CharacterOffsetBegin>\r",
      "\r\n",
      "            <CharacterOffsetEnd>7</CharacterOffsetEnd>\r",
      "\r\n",
      "            <POS>JJ</POS>\r",
      "\r\n",
      "            <NER>O</NER>\r",
      "\r\n",
      "            <Speaker>PER0</Speaker>\r",
      "\r\n",
      "          </token>\r",
      "\r\n",
      "          <token id=\"2\">\r",
      "\r\n",
      "            <word>language</word>\r",
      "\r\n",
      "            <lemma>language</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>8</CharacterOffsetBegin>\r",
      "\r\n",
      "            <CharacterOffsetEnd>16</CharacterOffsetEnd>\r",
      "\r\n",
      "            <POS>NN</POS>\r",
      "\r\n",
      "            <NER>O</NER>\r",
      "\r\n",
      "            <Speaker>PER0</Speaker>\r",
      "\r\n",
      "          </token>\r",
      "\r\n",
      "          <token id=\"3\">\r",
      "\r\n",
      "            <word>processing</word>\r",
      "\r\n",
      "            <lemma>processing</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>17</CharacterOffsetBegin>\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 work/nlp.txt.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "From\n",
      "Wikipedia\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "-LRB-\n",
      "NLP\n",
      "-RRB-\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from itertools import islice\n",
    "\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for word in islice(root.iter('word'), 20):   # Element.iter()  : 配下 (その子ノードや孫ノードなど) の部分木全体を再帰的にイテレートする\n",
    "    if not word.text in (',', '.'):\n",
    "        print(word.text)   #text化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tlemma\tPOS\n",
      "\n",
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "From\tfrom\tIN\n",
      "Wikipedia\tWikipedia\tNNP\n",
      "the\tthe\tDT\n",
      "free\tfree\tJJ\n",
      "encyclopedia\tencyclopedia\tNN\n",
      "Natural\tnatural\tJJ\n",
      "language\tlanguage\tNN\n",
      "processing\tprocessing\tNN\n",
      "-LRB-\t-lrb-\t-LRB-\n",
      "NLP\tnlp\tNN\n",
      "-RRB-\t-rrb-\t-RRB-\n",
      "is\tbe\tVBZ\n",
      "a\ta\tDT\n",
      "field\tfield\tNN\n",
      "of\tof\tIN\n",
      "computer\tcomputer\tNN\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from itertools import islice\n",
    "\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print('word\\tlemma\\tPOS\\n')\n",
    "for token in islice(root.iter('token'), 20):\n",
    "    if not token[0].text in (',', '.'):\n",
    "        print(\"{}\\t{}\\t{}\".format(token[0].text, token[1].text, token[4].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan\n",
      "Turing\n",
      "Joseph\n",
      "Weizenbaum\n",
      "MARGIE\n",
      "Schank\n",
      "Wilensky\n",
      "Meehan\n",
      "Lehnert\n",
      "Carbonell\n",
      "Lehnert\n",
      "Racter\n",
      "Jabberwacky\n",
      "Moore\n"
     ]
    }
   ],
   "source": [
    "# 人名どうやってとる？？？\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for token in root.findall('.//token[NER=\"PERSON\"]'):   # <NER>PERSON</NER>のとこを取ってるっぽい\n",
    "    print(token[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan Turing', 'Joseph Weizenbaum', 'MARGIE', 'Schank', 'Wilensky', 'Meehan', 'Lehnert', 'Carbonell', 'Lehnert', 'Racter', 'Jabberwacky', 'Moore']\n"
     ]
    }
   ],
   "source": [
    "#  栗林さん\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "list_persons = []\n",
    "person = \"\"\n",
    "previous_offset_end = 0\n",
    "\n",
    "for token in root.findall(\".//token[NER='PERSON']\"): \n",
    "    current_offset_start = int(token.find(\"CharacterOffsetBegin\").text)\n",
    "    \n",
    "    #前の単語と隣接していたら、personにconcatしていく\n",
    "    if current_offset_start == previous_offset_end + 1:\n",
    "        person = person + \" \" + token.find(\"word\").text\n",
    "    \n",
    "    #そうでなければ、personをlist_personsに追加\n",
    "    #personを新たなtokenで上書き\n",
    "    else:\n",
    "        if person:\n",
    "            list_persons.append(person)\n",
    "        person = token.find(\"word\").text\n",
    "\n",
    "    previous_offset_end = int(token.find(\"CharacterOffsetEnd\").text)\n",
    "    \n",
    "if person:\n",
    "    list_persons.append(person)\n",
    "        \n",
    "print(list_persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ~~↑他の文章の人名とconcatされることはない？？~~\n",
    "#### CharacterOffsetBegin/Endは文章が変わってもどんどん足されていくから，問題ない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan Turing', 'Joseph Weizenbaum', 'MARGIE', 'Schank', 'Wilensky', 'Meehan', 'Lehnert', 'Carbonell', 'Lehnert', 'Racter', 'Jabberwacky', 'Moore']\n"
     ]
    }
   ],
   "source": [
    "# findいらないと思って変えてみたけど読みづらくなった\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "persons_list = []\n",
    "person = ''\n",
    "previous_offset_end = 0\n",
    "\n",
    "for token in root.findall('.//token[NER=\"PERSON\"]'):\n",
    "    current_offset_start = int(token[2].text)\n",
    "    \n",
    "    if current_offset_start == previous_offset_end + 1:\n",
    "        person = person + ' ' + token[0].text\n",
    "    \n",
    "    else:\n",
    "        if person:\n",
    "            persons_list.append(person)\n",
    "        person = token[0].text\n",
    "\n",
    "    previous_offset_end = int(token[3].text)\n",
    "    \n",
    "if person:\n",
    "    persons_list.append(person)\n",
    "        \n",
    "print(persons_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### findallはリストで全部取ってきちゃうから，iterfindの方がいいかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan Turing', 'Joseph Weizenbaum', 'MARGIE', 'Schank', 'Wilensky', 'Meehan', 'Lehnert', 'Carbonell', 'Lehnert', 'Racter', 'Jabberwacky', 'Moore']\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "list_persons = []\n",
    "person = \"\"\n",
    "previous_offset_end = 0\n",
    "\n",
    "for token in root.iterfind(\".//token[NER='PERSON']\"): \n",
    "    current_offset_start = int(token.find(\"CharacterOffsetBegin\").text)\n",
    "    \n",
    "    if current_offset_start == previous_offset_end + 1:\n",
    "        person = person + \" \" + token.find(\"word\").text\n",
    "    \n",
    "    else:\n",
    "        if person:\n",
    "            list_persons.append(person)\n",
    "        person = token.find(\"word\").text\n",
    "\n",
    "    previous_offset_end = int(token.find(\"CharacterOffsetEnd\").text)\n",
    "    \n",
    "if person:\n",
    "    list_persons.append(person)\n",
    "        \n",
    "print(list_persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素人?の言語処理100本ノック:56\n",
    "https://qiita.com/segavvy/items/0340d3d71c9151265bcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 中身よく分かんない＼(*^◯^*)／\\nimport xml.etree.ElementTree as ET\\n\\ntree = ET.parse(\\'work/nlp.txt.xml\\')\\nroot = tree.getroot()\\n\\nfor coref in root.findall(\\'.//coreference/coreference\\'):\\n    rep_mention = coref.find(\"*[@representative=\\'true\\']\")\\n    print(rep_mention[0].text)\\n    print(rep_mention[1].text)\\n    print(rep_mention[2].text)\\n    print(rep_mention[3].text)\\n    print(rep_mention[4].text)\\n    print()\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 中身よく分かんない＼(*^◯^*)／\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for coref in root.findall('.//coreference/coreference'):\n",
    "    rep_mention = coref.find(\"*[@representative='true']\")\n",
    "    print(rep_mention[0].text)\n",
    "    print(rep_mention[1].text)\n",
    "    print(rep_mention[2].text)\n",
    "    print(rep_mention[3].text)\n",
    "    print(rep_mention[4].text)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <coreference>\r",
      "\r\n",
      "      <coreference>\r",
      "\r\n",
      "        <mention representative=\"true\">\r",
      "\r\n",
      "          <sentence>1</sentence>\r",
      "\r\n",
      "          <start>7</start>\r",
      "\r\n",
      "          <end>16</end>\r",
      "\r\n",
      "          <head>12</head>\r",
      "\r\n",
      "          <text>the free encyclopedia Natural language processing -LRB- NLP -RRB-</text>\r",
      "\r\n",
      "        </mention>\r",
      "\r\n",
      "        <mention>\r",
      "\r\n",
      "          <sentence>1</sentence>\r",
      "\r\n",
      "          <start>17</start>\r",
      "\r\n",
      "          <end>22</end>\r",
      "\r\n",
      "          <head>18</head>\r",
      "\r\n",
      "          <text>a field of computer science</text>\r",
      "\r\n",
      "        </mention>\r",
      "\r\n",
      "        <mention>\r",
      "\r\n",
      "          <sentence>18</sentence>\r",
      "\r\n",
      "          <start>23</start>\r",
      "\r\n",
      "          <end>25</end>\r",
      "\r\n",
      "          <head>24</head>\r",
      "\r\n",
      "          <text>language processing</text>\r",
      "\r\n",
      "--\r\n",
      "      <coreference>\r",
      "\r\n",
      "        <mention representative=\"true\">\r",
      "\r\n",
      "          <sentence>1</sentence>\r",
      "\r\n",
      "          <start>33</start>\r",
      "\r\n",
      "          <end>34</end>\r",
      "\r\n",
      "          <head>33</head>\r",
      "\r\n",
      "          <text>computers</text>\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -A 20 '<coreference>' work/nlp.txt.xml | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "+ 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "+ 主語: 述語からnsubj関係にある子（dependent）\n",
    "+ 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
